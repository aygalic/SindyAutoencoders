{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from example_thomas import get_thomas_data\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate training, validation, testing data\n",
    "noise_strength = 1e-6\n",
    "training_data = get_thomas_data(1024, noise_strength=noise_strength)\n",
    "validation_data = get_thomas_data(20, noise_strength=noise_strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['input_dim'] = 128\n",
    "params['latent_dim'] = 3\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 3\n",
    "params['include_sine'] = False\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], params['include_sine'], True)\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_z'] = 0.0\n",
    "params['loss_weight_sindy_x'] = 1e-4\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [64,32]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "params['batch_size'] = 1024\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 100\n",
    "params['print_frequency'] = 1 # for testing\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 5001 # WAY TOO LONG ON AN M1 MAX\n",
    "params['max_epochs'] = 1000\n",
    "params['refinement_epochs'] = 1001 # idk what this is \n",
    "params['refinement_epochs'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 20:38:26.061587: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-12 20:38:26.062467: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-01-12 20:38:26.141118: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-12 20:38:26.502273: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-12 20:38:37.113234: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "   training loss 0.18387821316719055, (0.18386748, 141.15512, 0.017425064, 0.89796865)\n",
      "   validation loss 0.226305291056633, (0.22629198, 110.724266, 0.043270793, 0.89796865)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 20:38:37.723271: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder loss ratio: 0.483884, decoder SINDy loss  ratio: 3.442894\n",
      "Epoch 1\n",
      "   training loss 0.18032748997211456, (0.18031718, 72.07468, 0.017060902, 0.8617107)\n",
      "   validation loss 0.2154376059770584, (0.21542646, 54.729374, 0.025268234, 0.8617107)\n",
      "decoder loss ratio: 0.460650, decoder SINDy loss  ratio: 2.010498\n",
      "Epoch 2\n",
      "   training loss 0.17431622743606567, (0.17430301, 58.40391, 0.049408995, 0.82658476)\n",
      "   validation loss 0.20572993159294128, (0.20571606, 53.079967, 0.055963945, 0.82658476)\n",
      "decoder loss ratio: 0.439886, decoder SINDy loss  ratio: 4.452840\n",
      "Epoch 3\n",
      "   training loss 0.10983040928840637, (0.10980943, 206.41997, 0.13243836, 0.7736687)\n",
      "   validation loss 0.11630702763795853, (0.116287425, 162.3036, 0.11866428, 0.7736687)\n",
      "decoder loss ratio: 0.248659, decoder SINDy loss  ratio: 9.441669\n",
      "Epoch 4\n",
      "   training loss 0.1054980605840683, (0.10548855, 81.902664, 0.018754398, 0.76261073)\n",
      "   validation loss 0.10532785207033157, (0.105317585, 46.66145, 0.026349248, 0.76261073)\n",
      "decoder loss ratio: 0.225202, decoder SINDy loss  ratio: 2.096510\n",
      "Epoch 5\n",
      "   training loss 0.10483307391405106, (0.10482464, 63.025963, 0.009201648, 0.75130326)\n",
      "   validation loss 0.10001980513334274, (0.10001045, 37.24558, 0.018494174, 0.75130326)\n",
      "decoder loss ratio: 0.213854, decoder SINDy loss  ratio: 1.471512\n",
      "Epoch 6\n",
      "   training loss 0.1044677346944809, (0.10445971, 37.222046, 0.0063703666, 0.73832685)\n",
      "   validation loss 0.09814015030860901, (0.098131105, 24.490082, 0.016624441, 0.73832685)\n",
      "decoder loss ratio: 0.209835, decoder SINDy loss  ratio: 1.322744\n",
      "Epoch 7\n",
      "   training loss 0.10434101521968842, (0.10433324, 22.96339, 0.0053908853, 0.7244403)\n",
      "   validation loss 0.09674528986215591, (0.096736394, 16.905766, 0.016533926, 0.7244403)\n",
      "decoder loss ratio: 0.206853, decoder SINDy loss  ratio: 1.315542\n",
      "Epoch 8\n",
      "   training loss 0.10409390926361084, (0.10408625, 15.509651, 0.0057884404, 0.7081322)\n",
      "   validation loss 0.0957942083477974, (0.09578524, 13.59789, 0.018907959, 0.7081322)\n",
      "decoder loss ratio: 0.204819, decoder SINDy loss  ratio: 1.504435\n",
      "Epoch 9\n",
      "   training loss 0.10352391749620438, (0.10351616, 10.761917, 0.008646702, 0.6893304)\n",
      "   validation loss 0.09484486281871796, (0.09483541, 11.862377, 0.02559685, 0.6893304)\n",
      "decoder loss ratio: 0.202788, decoder SINDy loss  ratio: 2.036645\n",
      "Epoch 10\n",
      "   training loss 0.10241290181875229, (0.102404565, 7.387571, 0.016612105, 0.6676739)\n",
      "   validation loss 0.09355418384075165, (0.09354321, 11.057882, 0.042969566, 0.6676739)\n",
      "decoder loss ratio: 0.200025, decoder SINDy loss  ratio: 3.418926\n",
      "Epoch 11\n",
      "   training loss 0.09870295971632004, (0.09869236, 4.8155246, 0.04212004, 0.6392463)\n",
      "   validation loss 0.09001599997282028, (0.089995496, 12.357048, 0.14112109, 0.6392463)\n",
      "decoder loss ratio: 0.192439, decoder SINDy loss  ratio: 11.228473\n",
      "Epoch 12\n",
      "   training loss 0.03722664713859558, (0.03719177, 11.413128, 0.29676044, 0.5199387)\n",
      "   validation loss 0.039845388382673264, (0.039337397, 48.874573, 5.027923, 0.5199387)\n",
      "decoder loss ratio: 0.084116, decoder SINDy loss  ratio: 400.052873\n",
      "Epoch 13\n",
      "   training loss 0.01119362935423851, (0.011178602, 2.7061915, 0.11097384, 0.39292413)\n",
      "   validation loss 0.017502864822745323, (0.017345196, 11.838578, 1.5374081, 0.39292413)\n",
      "decoder loss ratio: 0.037090, decoder SINDy loss  ratio: 122.325764\n",
      "Epoch 14\n",
      "   training loss 0.009052290581166744, (0.009040425, 1.0565022, 0.08508778, 0.33571225)\n",
      "   validation loss 0.013540037907660007, (0.013461279, 4.4998436, 0.7540169, 0.33571225)\n",
      "decoder loss ratio: 0.028784, decoder SINDy loss  ratio: 59.994278\n",
      "Epoch 15\n",
      "   training loss 0.007979951798915863, (0.007970715, 0.5295185, 0.0629374, 0.29432306)\n",
      "   validation loss 0.01156939473003149, (0.011520311, 2.391117, 0.46140078, 0.29432306)\n",
      "decoder loss ratio: 0.024634, decoder SINDy loss  ratio: 36.711919\n",
      "Epoch 16\n",
      "   training loss 0.007218392565846443, (0.007210535, 0.35568738, 0.05256607, 0.2601225)\n",
      "   validation loss 0.009933095425367355, (0.009897026, 1.576531, 0.33468255, 0.2601225)\n",
      "decoder loss ratio: 0.021163, decoder SINDy loss  ratio: 26.629428\n",
      "Epoch 17\n",
      "   training loss 0.0057840230874717236, (0.0057772975, 0.300871, 0.044363804, 0.22891103)\n",
      "   validation loss 0.008104872889816761, (0.008078034, 1.0959746, 0.24549893, 0.22891103)\n",
      "decoder loss ratio: 0.017273, decoder SINDy loss  ratio: 19.533423\n",
      "Epoch 18\n",
      "   training loss 0.00542279239743948, (0.005417129, 0.25059503, 0.036585193, 0.20048903)\n",
      "   validation loss 0.007177051156759262, (0.0071563185, 0.8096293, 0.18728183, 0.20048903)\n",
      "decoder loss ratio: 0.015302, decoder SINDy loss  ratio: 14.901309\n",
      "Epoch 19\n",
      "   training loss 0.0042966390028595924, (0.0042920774, 0.21841156, 0.028162114, 0.17451838)\n",
      "   validation loss 0.006091922987252474, (0.0060774046, 0.5411957, 0.1277307, 0.17451838)\n",
      "decoder loss ratio: 0.012995, decoder SINDy loss  ratio: 10.163050\n",
      "Epoch 20\n",
      "   training loss 0.004104097839444876, (0.0041004773, 0.1507507, 0.02103724, 0.15166977)\n",
      "   validation loss 0.005613483022898436, (0.0056028096, 0.38719064, 0.09156867, 0.15166977)\n",
      "decoder loss ratio: 0.011981, decoder SINDy loss  ratio: 7.285774\n",
      "Epoch 21\n",
      "   training loss 0.00413492601364851, (0.004131991, 0.10569503, 0.016262468, 0.13089404)\n",
      "   validation loss 0.005461841356009245, (0.0054544033, 0.26129243, 0.061289165, 0.13089404)\n",
      "decoder loss ratio: 0.011663, decoder SINDy loss  ratio: 4.876548\n",
      "Epoch 22\n",
      "   training loss 0.0038455622270703316, (0.0038431545, 0.07928806, 0.012884023, 0.11192961)\n",
      "   validation loss 0.005397974047809839, (0.005392634, 0.18255807, 0.042208448, 0.11192961)\n",
      "decoder loss ratio: 0.011531, decoder SINDy loss  ratio: 3.358367\n",
      "Epoch 23\n",
      "   training loss 0.0036496324464678764, (0.003647678, 0.05743715, 0.01007307, 0.094722554)\n",
      "   validation loss 0.00496663199737668, (0.004962808, 0.13040246, 0.02877069, 0.094722554)\n",
      "decoder loss ratio: 0.010612, decoder SINDy loss  ratio: 2.289175\n",
      "Epoch 24\n",
      "   training loss 0.003518114797770977, (0.003516531, 0.042733025, 0.007913699, 0.07922642)\n",
      "   validation loss 0.0048513151705265045, (0.0048485207, 0.09944064, 0.020024432, 0.07922642)\n",
      "decoder loss ratio: 0.010368, decoder SINDy loss  ratio: 1.593268\n",
      "Epoch 25\n",
      "   training loss 0.0032456563785672188, (0.0032444159, 0.03193211, 0.0058818865, 0.06523438)\n",
      "   validation loss 0.00443089846521616, (0.004428889, 0.08295224, 0.013571389, 0.06523438)\n",
      "decoder loss ratio: 0.009470, decoder SINDy loss  ratio: 1.079824\n",
      "Epoch 26\n",
      "   training loss 0.0029906630516052246, (0.0029897278, 0.023802465, 0.0040475265, 0.053066965)\n",
      "   validation loss 0.0040112766437232494, (0.00400983, 0.07295564, 0.009159626, 0.053066965)\n",
      "decoder loss ratio: 0.008574, decoder SINDy loss  ratio: 0.728797\n",
      "Epoch 27\n",
      "   training loss 0.002584623172879219, (0.0025839114, 0.018085478, 0.0028420219, 0.042745862)\n",
      "   validation loss 0.00351051171310246, (0.0035094852, 0.057566393, 0.0059899543, 0.042745862)\n",
      "decoder loss ratio: 0.007504, decoder SINDy loss  ratio: 0.476598\n",
      "Epoch 28\n",
      "   training loss 0.0024222901556640863, (0.002421745, 0.015444748, 0.0019900524, 0.03460818)\n",
      "   validation loss 0.0034014442935585976, (0.003400688, 0.044799477, 0.004102919, 0.03460818)\n",
      "decoder loss ratio: 0.007272, decoder SINDy loss  ratio: 0.326454\n",
      "Epoch 29\n",
      "   training loss 0.0019260105909779668, (0.0019255815, 0.012083053, 0.0014945755, 0.02796132)\n",
      "   validation loss 0.002799293491989374, (0.0027987338, 0.031920888, 0.0028000847, 0.02796132)\n",
      "decoder loss ratio: 0.005985, decoder SINDy loss  ratio: 0.222792\n",
      "Epoch 30\n",
      "   training loss 0.0018988169031217694, (0.0018984787, 0.010301443, 0.0011388923, 0.022438642)\n",
      "   validation loss 0.0029679674189537764, (0.002967516, 0.02311603, 0.002269809, 0.022438642)\n",
      "decoder loss ratio: 0.006345, decoder SINDy loss  ratio: 0.180600\n",
      "Epoch 31\n",
      "   training loss 0.001361351809464395, (0.0013610697, 0.00741055, 0.00096657407, 0.018539945)\n",
      "   validation loss 0.002211155602708459, (0.0022107826, 0.017945625, 0.0018765999, 0.018539945)\n",
      "decoder loss ratio: 0.004727, decoder SINDy loss  ratio: 0.149314\n",
      "Epoch 32\n",
      "   training loss 0.001520613324828446, (0.0015203655, 0.006529502, 0.0008360774, 0.01643159)\n",
      "   validation loss 0.002636623103171587, (0.0026362853, 0.015257318, 0.0017352705, 0.01643159)\n",
      "decoder loss ratio: 0.005637, decoder SINDy loss  ratio: 0.138069\n",
      "Epoch 33\n",
      "   training loss 0.0009963695192709565, (0.000996139, 0.0051681334, 0.00079560094, 0.015103647)\n",
      "   validation loss 0.0018841176060959697, (0.0018838069, 0.013591019, 0.0015972967, 0.015103647)\n",
      "decoder loss ratio: 0.004028, decoder SINDy loss  ratio: 0.127091\n",
      "Epoch 34\n",
      "   training loss 0.0012002226430922747, (0.0012000094, 0.0044552535, 0.0007338282, 0.013997494)\n",
      "   validation loss 0.00214561284519732, (0.0021453162, 0.0132993525, 0.0015664749, 0.013997494)\n",
      "decoder loss ratio: 0.004587, decoder SINDy loss  ratio: 0.124638\n",
      "Epoch 35\n",
      "   training loss 0.0008195522241294384, (0.00081934803, 0.0035693038, 0.0007163117, 0.013250994)\n",
      "   validation loss 0.0015773227205500007, (0.0015770372, 0.012384652, 0.001531243, 0.013250994)\n",
      "decoder loss ratio: 0.003372, decoder SINDy loss  ratio: 0.121835\n",
      "Epoch 36\n",
      "   training loss 0.0011040898971259594, (0.001103894, 0.0030921488, 0.00068120926, 0.012785869)\n",
      "   validation loss 0.0018722180975601077, (0.001871939, 0.012373366, 0.0015118201, 0.012785869)\n",
      "decoder loss ratio: 0.004003, decoder SINDy loss  ratio: 0.120290\n",
      "Epoch 37\n",
      "   training loss 0.0007685463642701507, (0.0007683507, 0.0026684883, 0.000681413, 0.012747552)\n",
      "   validation loss 0.0014377529732882977, (0.001437475, 0.011092809, 0.0015052976, 0.012747552)\n",
      "decoder loss ratio: 0.003074, decoder SINDy loss  ratio: 0.119771\n",
      "Epoch 38\n",
      "   training loss 0.0011723577044904232, (0.0011721634, 0.002330344, 0.0006785798, 0.012647167)\n",
      "   validation loss 0.002040280494838953, (0.0020400065, 0.011310687, 0.0014749906, 0.012647167)\n",
      "decoder loss ratio: 0.004362, decoder SINDy loss  ratio: 0.117359\n",
      "Epoch 39\n",
      "   training loss 0.0008220369345508516, (0.0008218426, 0.0021295766, 0.0006761541, 0.012670215)\n",
      "   validation loss 0.0015883439918980002, (0.0015880711, 0.010436412, 0.0014626308, 0.012670215)\n",
      "decoder loss ratio: 0.003396, decoder SINDy loss  ratio: 0.116376\n",
      "Epoch 40\n",
      "   training loss 0.0009677975904196501, (0.0009676033, 0.0019616063, 0.00067889446, 0.012643861)\n",
      "   validation loss 0.0018940543523058295, (0.0018937845, 0.010445266, 0.001434627, 0.012643861)\n",
      "decoder loss ratio: 0.004050, decoder SINDy loss  ratio: 0.114148\n",
      "Epoch 41\n",
      "   training loss 0.0008736011805012822, (0.000873408, 0.0018660523, 0.0006710095, 0.012607007)\n",
      "   validation loss 0.0018013729713857174, (0.0018011036, 0.010286537, 0.0014336281, 0.012607007)\n",
      "decoder loss ratio: 0.003851, decoder SINDy loss  ratio: 0.114068\n",
      "Epoch 42\n",
      "   training loss 0.0008031412726268172, (0.0008029486, 0.0017641404, 0.0006668444, 0.012593566)\n",
      "   validation loss 0.001730879070237279, (0.0017306114, 0.009697753, 0.0014165617, 0.012593566)\n",
      "decoder loss ratio: 0.003701, decoder SINDy loss  ratio: 0.112710\n",
      "Epoch 43\n",
      "   training loss 0.0008142683655023575, (0.0008140772, 0.0017041145, 0.0006571914, 0.012542012)\n",
      "   validation loss 0.0018008667975664139, (0.0018006006, 0.009670975, 0.0014081205, 0.012542012)\n",
      "decoder loss ratio: 0.003850, decoder SINDy loss  ratio: 0.112039\n",
      "Epoch 44\n",
      "   training loss 0.0007377360016107559, (0.00073754584, 0.0016397024, 0.00064932584, 0.012521745)\n",
      "   validation loss 0.0017047973815351725, (0.0017045331, 0.009075672, 0.0013905279, 0.012521745)\n",
      "decoder loss ratio: 0.003645, decoder SINDy loss  ratio: 0.110639\n",
      "Epoch 45\n",
      "   training loss 0.0006828751065768301, (0.00068268616, 0.001605053, 0.00064357254, 0.012455361)\n",
      "   validation loss 0.0017111622728407383, (0.0017108986, 0.008888664, 0.0013913334, 0.012455361)\n",
      "decoder loss ratio: 0.003658, decoder SINDy loss  ratio: 0.110703\n",
      "Epoch 46\n",
      "   training loss 0.000595122401136905, (0.0005949348, 0.0015587893, 0.0006321999, 0.012439349)\n",
      "   validation loss 0.0016125421971082687, (0.0016122791, 0.0077233673, 0.0013864605, 0.012439349)\n",
      "decoder loss ratio: 0.003448, decoder SINDy loss  ratio: 0.110315\n",
      "Epoch 47\n",
      "   training loss 0.0005679210880771279, (0.0005677334, 0.0015756296, 0.0006345834, 0.012423386)\n",
      "   validation loss 0.001757521997205913, (0.0017572604, 0.0078789815, 0.0013736711, 0.012423386)\n",
      "decoder loss ratio: 0.003758, decoder SINDy loss  ratio: 0.109298\n",
      "Epoch 48\n",
      "   training loss 0.0005722084315493703, (0.00057202147, 0.0016172355, 0.0006299466, 0.012400797)\n",
      "   validation loss 0.0014768195105716586, (0.001476556, 0.0076020127, 0.0013954882, 0.012400797)\n",
      "decoder loss ratio: 0.003157, decoder SINDy loss  ratio: 0.111034\n",
      "Epoch 49\n",
      "   training loss 0.0005949573824182153, (0.00059477147, 0.0016248705, 0.0006295205, 0.012292033)\n",
      "   validation loss 0.0016804615734145045, (0.0016802013, 0.007593032, 0.0013742127, 0.012292033)\n",
      "decoder loss ratio: 0.003593, decoder SINDy loss  ratio: 0.109341\n",
      "Epoch 50\n",
      "   training loss 0.0005155951948836446, (0.00051540905, 0.0016398998, 0.00063144777, 0.01230142)\n",
      "   validation loss 0.0016169442096725106, (0.0016166837, 0.00719721, 0.0013744268, 0.01230142)\n",
      "decoder loss ratio: 0.003457, decoder SINDy loss  ratio: 0.109358\n",
      "Epoch 51\n",
      "   training loss 0.0004484452947508544, (0.00044825967, 0.001656406, 0.00062347506, 0.012327984)\n",
      "   validation loss 0.001444677822291851, (0.0014444187, 0.007051576, 0.0013585878, 0.012327984)\n",
      "decoder loss ratio: 0.003089, decoder SINDy loss  ratio: 0.108098\n",
      "Epoch 52\n",
      "   training loss 0.0005062442505732179, (0.000506059, 0.0016997874, 0.0006252766, 0.012274403)\n",
      "   validation loss 0.0017369079869240522, (0.0017366508, 0.0066712606, 0.0013443733, 0.012274403)\n",
      "decoder loss ratio: 0.003714, decoder SINDy loss  ratio: 0.106967\n",
      "Epoch 53\n",
      "   training loss 0.0003711477038450539, (0.00037096022, 0.0016996003, 0.00063721946, 0.012377801)\n",
      "   validation loss 0.0012168831890448928, (0.0012166307, 0.0066680666, 0.0012872261, 0.012377801)\n",
      "decoder loss ratio: 0.002602, decoder SINDy loss  ratio: 0.102420\n",
      "Epoch 54\n",
      "   training loss 0.0007030565175227821, (0.00070287025, 0.0017868903, 0.00062773586, 0.012354141)\n",
      "   validation loss 0.0020793387666344643, (0.0020790852, 0.0066513754, 0.0013001398, 0.012354141)\n",
      "decoder loss ratio: 0.004446, decoder SINDy loss  ratio: 0.103447\n",
      "Epoch 55\n",
      "   training loss 0.00024284386017825454, (0.00024264482, 0.001819032, 0.0007173458, 0.0127306)\n",
      "   validation loss 0.0007600742392241955, (0.0007598055, 0.0058263205, 0.0014146516, 0.0127306)\n",
      "decoder loss ratio: 0.001625, decoder SINDy loss  ratio: 0.112558\n",
      "Epoch 56\n",
      "   training loss 0.0003989876131527126, (0.00039879672, 0.0017698505, 0.00065659627, 0.012522623)\n",
      "   validation loss 0.0006109637906774879, (0.00061070494, 0.00572488, 0.0013365828, 0.012522623)\n",
      "decoder loss ratio: 0.001306, decoder SINDy loss  ratio: 0.106347\n"
     ]
    }
   ],
   "source": [
    "num_experiments = 1\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_experiments):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "\n",
    "    params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "\n",
    "    params['save_name'] = 'lorenz_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "\n",
    "df.to_pickle('experiment_results_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0d66913bb719389121bab79020f7a52c6e05d87274bae621f301af382c19c893"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
